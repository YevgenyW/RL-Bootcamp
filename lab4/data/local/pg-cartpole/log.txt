[2019-08-24 14:36:48.101735 UTC] Starting env pool
[2019-08-24 14:36:48.185814 UTC] Starting iteration 0
[2019-08-24 14:36:48.187653 UTC] Start collecting samples
[2019-08-24 14:36:49.263176 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:49.324428 UTC] Computing policy gradient
[2019-08-24 14:36:49.367281 UTC] Updating baseline
[2019-08-24 14:36:49.491769 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| SurrLoss             | -0.0026496 |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2019-08-24 14:36:50.190711 UTC] Saving snapshot
[2019-08-24 14:36:50.203991 UTC] Starting iteration 1
[2019-08-24 14:36:50.205011 UTC] Start collecting samples
[2019-08-24 14:36:50.769969 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:50.826724 UTC] Computing policy gradient
[2019-08-24 14:36:50.838012 UTC] Updating baseline
[2019-08-24 14:36:50.952012 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| SurrLoss             | -0.028403 |
| Entropy              | 0.63881   |
| Perplexity           | 1.8942    |
| AveragePolicyProb[0] | 0.48601   |
| AveragePolicyProb[1] | 0.51399   |
| AverageReturn        | 30.72     |
| MinReturn            | 9         |
| MaxReturn            | 109       |
| StdReturn            | 18.103    |
| AverageEpisodeLength | 30.72     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 109       |
| StdEpisodeLength     | 18.103    |
| TotalNEpisodes       | 124       |
| TotalNSamples        | 3619      |
| ExplainedVariance    | 0.15902   |
------------------------------------
[2019-08-24 14:36:51.659348 UTC] Saving snapshot
[2019-08-24 14:36:51.672837 UTC] Starting iteration 2
[2019-08-24 14:36:51.674443 UTC] Start collecting samples
[2019-08-24 14:36:52.081788 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:52.103248 UTC] Computing policy gradient
[2019-08-24 14:36:52.113271 UTC] Updating baseline
[2019-08-24 14:36:52.219684 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| SurrLoss             | -0.044707 |
| Entropy              | 0.60104   |
| Perplexity           | 1.824     |
| AveragePolicyProb[0] | 0.48011   |
| AveragePolicyProb[1] | 0.51989   |
| AverageReturn        | 38.42     |
| MinReturn            | 10        |
| MaxReturn            | 112       |
| StdReturn            | 22.32     |
| AverageEpisodeLength | 38.42     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 112       |
| StdEpisodeLength     | 22.32     |
| TotalNEpisodes       | 148       |
| TotalNSamples        | 5017      |
| ExplainedVariance    | 0.33974   |
------------------------------------
[2019-08-24 14:36:53.172927 UTC] Saving snapshot
[2019-08-24 14:36:53.184953 UTC] Starting iteration 3
[2019-08-24 14:36:53.186627 UTC] Start collecting samples
[2019-08-24 14:36:53.520175 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:53.540759 UTC] Computing policy gradient
[2019-08-24 14:36:53.551284 UTC] Updating baseline
[2019-08-24 14:36:53.653819 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| SurrLoss             | -0.021752 |
| Entropy              | 0.56557   |
| Perplexity           | 1.7605    |
| AveragePolicyProb[0] | 0.51612   |
| AveragePolicyProb[1] | 0.48388   |
| AverageReturn        | 53.1      |
| MinReturn            | 10        |
| MaxReturn            | 200       |
| StdReturn            | 42.011    |
| AverageEpisodeLength | 53.1      |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 42.011    |
| TotalNEpisodes       | 161       |
| TotalNSamples        | 6783      |
| ExplainedVariance    | 0.33004   |
------------------------------------
[2019-08-24 14:36:54.547132 UTC] Saving snapshot
[2019-08-24 14:36:54.560080 UTC] Starting iteration 4
[2019-08-24 14:36:54.561425 UTC] Start collecting samples
[2019-08-24 14:36:54.878072 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:54.895412 UTC] Computing policy gradient
[2019-08-24 14:36:54.902446 UTC] Updating baseline
[2019-08-24 14:36:55.012968 UTC] Computing logging information
-----------------------------------
| Iteration            | 4        |
| SurrLoss             | -0.01343 |
| Entropy              | 0.52271  |
| Perplexity           | 1.6866   |
| AveragePolicyProb[0] | 0.49949  |
| AveragePolicyProb[1] | 0.50051  |
| AverageReturn        | 68.93    |
| MinReturn            | 10       |
| MaxReturn            | 200      |
| StdReturn            | 52.911   |
| AverageEpisodeLength | 68.93    |
| MinEpisodeLength     | 10       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 52.911   |
| TotalNEpisodes       | 173      |
| TotalNSamples        | 8606     |
| ExplainedVariance    | 0.76978  |
-----------------------------------
[2019-08-24 14:36:55.937837 UTC] Saving snapshot
[2019-08-24 14:36:55.950915 UTC] Starting iteration 5
[2019-08-24 14:36:55.951876 UTC] Start collecting samples
[2019-08-24 14:36:56.240184 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:56.255614 UTC] Computing policy gradient
[2019-08-24 14:36:56.263876 UTC] Updating baseline
[2019-08-24 14:36:56.350233 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| SurrLoss             | -0.011786 |
| Entropy              | 0.48944   |
| Perplexity           | 1.6314    |
| AveragePolicyProb[0] | 0.50122   |
| AveragePolicyProb[1] | 0.49878   |
| AverageReturn        | 84.48     |
| MinReturn            | 16        |
| MaxReturn            | 200       |
| StdReturn            | 59.894    |
| AverageEpisodeLength | 84.48     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.894    |
| TotalNEpisodes       | 183       |
| TotalNSamples        | 10391     |
| ExplainedVariance    | 0.72004   |
------------------------------------
[2019-08-24 14:36:57.053353 UTC] Saving snapshot
[2019-08-24 14:36:57.064647 UTC] Starting iteration 6
[2019-08-24 14:36:57.065655 UTC] Start collecting samples
[2019-08-24 14:36:57.394370 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:57.412692 UTC] Computing policy gradient
[2019-08-24 14:36:57.422856 UTC] Updating baseline
[2019-08-24 14:36:57.525338 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| SurrLoss             | -0.023091 |
| Entropy              | 0.45278   |
| Perplexity           | 1.5727    |
| AveragePolicyProb[0] | 0.492     |
| AveragePolicyProb[1] | 0.508     |
| AverageReturn        | 102.91    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 62.442    |
| AverageEpisodeLength | 102.91    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 62.442    |
| TotalNEpisodes       | 197       |
| TotalNSamples        | 12648     |
| ExplainedVariance    | 0.67661   |
------------------------------------
[2019-08-24 14:36:58.216704 UTC] Saving snapshot
[2019-08-24 14:36:58.226931 UTC] Starting iteration 7
[2019-08-24 14:36:58.227828 UTC] Start collecting samples
[2019-08-24 14:36:58.554121 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:58.573708 UTC] Computing policy gradient
[2019-08-24 14:36:58.582396 UTC] Updating baseline
[2019-08-24 14:36:58.680780 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| SurrLoss             | -0.016464 |
| Entropy              | 0.42004   |
| Perplexity           | 1.522     |
| AveragePolicyProb[0] | 0.50509   |
| AveragePolicyProb[1] | 0.49491   |
| AverageReturn        | 119.87    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 61.119    |
| AverageEpisodeLength | 119.87    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 61.119    |
| TotalNEpisodes       | 211       |
| TotalNSamples        | 14932     |
| ExplainedVariance    | 0.67818   |
------------------------------------
[2019-08-24 14:36:59.379551 UTC] Saving snapshot
[2019-08-24 14:36:59.391080 UTC] Starting iteration 8
[2019-08-24 14:36:59.392168 UTC] Start collecting samples
[2019-08-24 14:36:59.668429 UTC] Computing input variables for policy optimization
[2019-08-24 14:36:59.683071 UTC] Computing policy gradient
[2019-08-24 14:36:59.691002 UTC] Updating baseline
[2019-08-24 14:36:59.932751 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| SurrLoss             | 0.0022306 |
| Entropy              | 0.3878    |
| Perplexity           | 1.4737    |
| AveragePolicyProb[0] | 0.50556   |
| AveragePolicyProb[1] | 0.49444   |
| AverageReturn        | 128.31    |
| MinReturn            | 29        |
| MaxReturn            | 200       |
| StdReturn            | 59.07     |
| AverageEpisodeLength | 128.31    |
| MinEpisodeLength     | 29        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.07     |
| TotalNEpisodes       | 219       |
| TotalNSamples        | 16195     |
| ExplainedVariance    | 0.76633   |
------------------------------------
[2019-08-24 14:37:00.733413 UTC] Saving snapshot
[2019-08-24 14:37:00.748855 UTC] Starting iteration 9
[2019-08-24 14:37:00.749860 UTC] Start collecting samples
[2019-08-24 14:37:01.082818 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:01.108020 UTC] Computing policy gradient
[2019-08-24 14:37:01.117297 UTC] Updating baseline
[2019-08-24 14:37:01.220205 UTC] Computing logging information
-------------------------------------
| Iteration            | 9          |
| SurrLoss             | -0.0028893 |
| Entropy              | 0.36246    |
| Perplexity           | 1.4369     |
| AveragePolicyProb[0] | 0.5021     |
| AveragePolicyProb[1] | 0.4979     |
| AverageReturn        | 142.68     |
| MinReturn            | 29         |
| MaxReturn            | 200        |
| StdReturn            | 54.707     |
| AverageEpisodeLength | 142.68     |
| MinEpisodeLength     | 29         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 54.707     |
| TotalNEpisodes       | 230        |
| TotalNSamples        | 18204      |
| ExplainedVariance    | 0.82493    |
-------------------------------------
[2019-08-24 14:37:01.908082 UTC] Saving snapshot
[2019-08-24 14:37:01.918640 UTC] Starting iteration 10
[2019-08-24 14:37:01.921645 UTC] Start collecting samples
[2019-08-24 14:37:02.268149 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:02.291437 UTC] Computing policy gradient
[2019-08-24 14:37:02.299749 UTC] Updating baseline
[2019-08-24 14:37:02.394584 UTC] Computing logging information
-----------------------------------
| Iteration            | 10       |
| SurrLoss             | 0.014146 |
| Entropy              | 0.33789  |
| Perplexity           | 1.402    |
| AveragePolicyProb[0] | 0.51755  |
| AveragePolicyProb[1] | 0.48245  |
| AverageReturn        | 161.44   |
| MinReturn            | 33       |
| MaxReturn            | 200      |
| StdReturn            | 45.801   |
| AverageEpisodeLength | 161.44   |
| MinEpisodeLength     | 33       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 45.801   |
| TotalNEpisodes       | 244      |
| TotalNSamples        | 20963    |
| ExplainedVariance    | 0.68108  |
-----------------------------------
[2019-08-24 14:37:03.521908 UTC] Saving snapshot
[2019-08-24 14:37:03.535778 UTC] Starting iteration 11
[2019-08-24 14:37:03.536803 UTC] Start collecting samples
[2019-08-24 14:37:03.803060 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:03.819140 UTC] Computing policy gradient
[2019-08-24 14:37:03.828922 UTC] Updating baseline
[2019-08-24 14:37:03.931755 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| SurrLoss             | -0.010883 |
| Entropy              | 0.32872   |
| Perplexity           | 1.3892    |
| AveragePolicyProb[0] | 0.50891   |
| AveragePolicyProb[1] | 0.49109   |
| AverageReturn        | 168.99    |
| MinReturn            | 64        |
| MaxReturn            | 200       |
| StdReturn            | 38.386    |
| AverageEpisodeLength | 168.99    |
| MinEpisodeLength     | 64        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 38.386    |
| TotalNEpisodes       | 250       |
| TotalNSamples        | 22163     |
| ExplainedVariance    | 0.91311   |
------------------------------------
[2019-08-24 14:37:05.088939 UTC] Saving snapshot
[2019-08-24 14:37:05.102666 UTC] Starting iteration 12
[2019-08-24 14:37:05.104757 UTC] Start collecting samples
[2019-08-24 14:37:05.406639 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:05.422338 UTC] Computing policy gradient
[2019-08-24 14:37:05.430419 UTC] Updating baseline
[2019-08-24 14:37:05.538317 UTC] Computing logging information
-----------------------------------
| Iteration            | 12       |
| SurrLoss             | 0.020091 |
| Entropy              | 0.30299  |
| Perplexity           | 1.3539   |
| AveragePolicyProb[0] | 0.50256  |
| AveragePolicyProb[1] | 0.49744  |
| AverageReturn        | 175.57   |
| MinReturn            | 64       |
| MaxReturn            | 200      |
| StdReturn            | 35.272   |
| AverageEpisodeLength | 175.57   |
| MinEpisodeLength     | 64       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 35.272   |
| TotalNEpisodes       | 260      |
| TotalNSamples        | 24140    |
| ExplainedVariance    | 0.56413  |
-----------------------------------
[2019-08-24 14:37:06.677458 UTC] Saving snapshot
[2019-08-24 14:37:06.693002 UTC] Starting iteration 13
[2019-08-24 14:37:06.694007 UTC] Start collecting samples
[2019-08-24 14:37:07.044335 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:07.062239 UTC] Computing policy gradient
[2019-08-24 14:37:07.069478 UTC] Updating baseline
[2019-08-24 14:37:07.158740 UTC] Computing logging information
-----------------------------------
| Iteration            | 13       |
| SurrLoss             | 0.013758 |
| Entropy              | 0.29758  |
| Perplexity           | 1.3466   |
| AveragePolicyProb[0] | 0.51499  |
| AveragePolicyProb[1] | 0.48501  |
| AverageReturn        | 179.78   |
| MinReturn            | 80       |
| MaxReturn            | 200      |
| StdReturn            | 31.141   |
| AverageEpisodeLength | 179.78   |
| MinEpisodeLength     | 80       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 31.141   |
| TotalNEpisodes       | 273      |
| TotalNSamples        | 26584    |
| ExplainedVariance    | 0.77148  |
-----------------------------------
[2019-08-24 14:37:08.293937 UTC] Saving snapshot
[2019-08-24 14:37:08.308376 UTC] Starting iteration 14
[2019-08-24 14:37:08.309423 UTC] Start collecting samples
[2019-08-24 14:37:08.618640 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:08.634834 UTC] Computing policy gradient
[2019-08-24 14:37:08.642952 UTC] Updating baseline
[2019-08-24 14:37:08.738482 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| SurrLoss             | 0.0056843 |
| Entropy              | 0.29567   |
| Perplexity           | 1.344     |
| AveragePolicyProb[0] | 0.53547   |
| AveragePolicyProb[1] | 0.46453   |
| AverageReturn        | 180.73    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 30.907    |
| AverageEpisodeLength | 180.73    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.907    |
| TotalNEpisodes       | 284       |
| TotalNSamples        | 28630     |
| ExplainedVariance    | 0.81334   |
------------------------------------
[2019-08-24 14:37:09.832689 UTC] Saving snapshot
[2019-08-24 14:37:09.847332 UTC] Starting iteration 15
[2019-08-24 14:37:09.848919 UTC] Start collecting samples
[2019-08-24 14:37:10.237674 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:10.258415 UTC] Computing policy gradient
[2019-08-24 14:37:10.267179 UTC] Updating baseline
[2019-08-24 14:37:10.389188 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| SurrLoss             | 0.0039076 |
| Entropy              | 0.29026   |
| Perplexity           | 1.3368    |
| AveragePolicyProb[0] | 0.53314   |
| AveragePolicyProb[1] | 0.46686   |
| AverageReturn        | 182.4     |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 26.478    |
| AverageEpisodeLength | 182.4     |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 26.478    |
| TotalNEpisodes       | 296       |
| TotalNSamples        | 30720     |
| ExplainedVariance    | 0.89693   |
------------------------------------
[2019-08-24 14:37:11.151252 UTC] Saving snapshot
[2019-08-24 14:37:11.165920 UTC] Starting iteration 16
[2019-08-24 14:37:11.166854 UTC] Start collecting samples
[2019-08-24 14:37:11.515986 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:11.534612 UTC] Computing policy gradient
[2019-08-24 14:37:11.543352 UTC] Updating baseline
[2019-08-24 14:37:11.640602 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| SurrLoss             | -0.018331 |
| Entropy              | 0.29744   |
| Perplexity           | 1.3464    |
| AveragePolicyProb[0] | 0.51573   |
| AveragePolicyProb[1] | 0.48427   |
| AverageReturn        | 185.31    |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 22.824    |
| AverageEpisodeLength | 185.31    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 22.824    |
| TotalNEpisodes       | 307       |
| TotalNSamples        | 32743     |
| ExplainedVariance    | 0.88666   |
------------------------------------
[2019-08-24 14:37:12.393503 UTC] Saving snapshot
[2019-08-24 14:37:12.408519 UTC] Starting iteration 17
[2019-08-24 14:37:12.410036 UTC] Start collecting samples
[2019-08-24 14:37:12.716496 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:12.732322 UTC] Computing policy gradient
[2019-08-24 14:37:12.739854 UTC] Updating baseline
[2019-08-24 14:37:12.840728 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| SurrLoss             | -0.023674 |
| Entropy              | 0.29992   |
| Perplexity           | 1.3498    |
| AveragePolicyProb[0] | 0.49612   |
| AveragePolicyProb[1] | 0.50388   |
| AverageReturn        | 188.4     |
| MinReturn            | 106       |
| MaxReturn            | 200       |
| StdReturn            | 19.282    |
| AverageEpisodeLength | 188.4     |
| MinEpisodeLength     | 106       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.282    |
| TotalNEpisodes       | 315       |
| TotalNSamples        | 34343     |
| ExplainedVariance    | 0.73718   |
------------------------------------
[2019-08-24 14:37:13.834365 UTC] Saving snapshot
[2019-08-24 14:37:13.846779 UTC] Starting iteration 18
[2019-08-24 14:37:13.849230 UTC] Start collecting samples
[2019-08-24 14:37:14.148462 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:14.175542 UTC] Computing policy gradient
[2019-08-24 14:37:14.190153 UTC] Updating baseline
[2019-08-24 14:37:14.310178 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| SurrLoss             | 0.0011825 |
| Entropy              | 0.28939   |
| Perplexity           | 1.3356    |
| AveragePolicyProb[0] | 0.49316   |
| AveragePolicyProb[1] | 0.50684   |
| AverageReturn        | 190.43    |
| MinReturn            | 131       |
| MaxReturn            | 200       |
| StdReturn            | 17.027    |
| AverageEpisodeLength | 190.43    |
| MinEpisodeLength     | 131       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.027    |
| TotalNEpisodes       | 324       |
| TotalNSamples        | 36143     |
| ExplainedVariance    | 0.57379   |
------------------------------------
[2019-08-24 14:37:15.573887 UTC] Saving snapshot
[2019-08-24 14:37:15.600305 UTC] Starting iteration 19
[2019-08-24 14:37:15.601296 UTC] Start collecting samples
[2019-08-24 14:37:15.953085 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:15.971564 UTC] Computing policy gradient
[2019-08-24 14:37:15.980854 UTC] Updating baseline
[2019-08-24 14:37:16.095825 UTC] Computing logging information
-------------------------------------
| Iteration            | 19         |
| SurrLoss             | -0.0085643 |
| Entropy              | 0.28966    |
| Perplexity           | 1.336      |
| AveragePolicyProb[0] | 0.50282    |
| AveragePolicyProb[1] | 0.49718    |
| AverageReturn        | 191.76     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.87      |
| AverageEpisodeLength | 191.76     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.87      |
| TotalNEpisodes       | 336        |
| TotalNSamples        | 38543      |
| ExplainedVariance    | 0.355      |
-------------------------------------
[2019-08-24 14:37:16.981469 UTC] Saving snapshot
[2019-08-24 14:37:16.994754 UTC] Starting iteration 20
[2019-08-24 14:37:16.996068 UTC] Start collecting samples
[2019-08-24 14:37:17.345435 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:17.367515 UTC] Computing policy gradient
[2019-08-24 14:37:17.379545 UTC] Updating baseline
[2019-08-24 14:37:17.475565 UTC] Computing logging information
--------------------------------------
| Iteration            | 20          |
| SurrLoss             | -0.00011771 |
| Entropy              | 0.27784     |
| Perplexity           | 1.3203      |
| AveragePolicyProb[0] | 0.49682     |
| AveragePolicyProb[1] | 0.50318     |
| AverageReturn        | 191.8       |
| MinReturn            | 144         |
| MaxReturn            | 200         |
| StdReturn            | 15.886      |
| AverageEpisodeLength | 191.8       |
| MinEpisodeLength     | 144         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 15.886      |
| TotalNEpisodes       | 344         |
| TotalNSamples        | 40143       |
| ExplainedVariance    | 0.58615     |
--------------------------------------
[2019-08-24 14:37:18.449054 UTC] Saving snapshot
[2019-08-24 14:37:18.460750 UTC] Starting iteration 21
[2019-08-24 14:37:18.461708 UTC] Start collecting samples
[2019-08-24 14:37:18.752851 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:18.769992 UTC] Computing policy gradient
[2019-08-24 14:37:18.777617 UTC] Updating baseline
[2019-08-24 14:37:18.899780 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| SurrLoss             | -0.0073582 |
| Entropy              | 0.2709     |
| Perplexity           | 1.3111     |
| AveragePolicyProb[0] | 0.5097     |
| AveragePolicyProb[1] | 0.49031    |
| AverageReturn        | 191.95     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.892     |
| AverageEpisodeLength | 191.95     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.892     |
| TotalNEpisodes       | 356        |
| TotalNSamples        | 42543      |
| ExplainedVariance    | 0.11105    |
-------------------------------------
[2019-08-24 14:37:19.775326 UTC] Saving snapshot
[2019-08-24 14:37:19.786895 UTC] Starting iteration 22
[2019-08-24 14:37:19.787796 UTC] Start collecting samples
[2019-08-24 14:37:20.072633 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:20.092455 UTC] Computing policy gradient
[2019-08-24 14:37:20.101347 UTC] Updating baseline
[2019-08-24 14:37:20.208471 UTC] Computing logging information
-------------------------------------
| Iteration            | 22         |
| SurrLoss             | 6.8436e-06 |
| Entropy              | 0.2683     |
| Perplexity           | 1.3077     |
| AveragePolicyProb[0] | 0.50357    |
| AveragePolicyProb[1] | 0.49643    |
| AverageReturn        | 192.03     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.912     |
| AverageEpisodeLength | 192.03     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.912     |
| TotalNEpisodes       | 365        |
| TotalNSamples        | 44343      |
| ExplainedVariance    | 0.45192    |
-------------------------------------
[2019-08-24 14:37:21.213362 UTC] Saving snapshot
[2019-08-24 14:37:21.229716 UTC] Starting iteration 23
[2019-08-24 14:37:21.230739 UTC] Start collecting samples
[2019-08-24 14:37:21.569393 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:21.589415 UTC] Computing policy gradient
[2019-08-24 14:37:21.600982 UTC] Updating baseline
[2019-08-24 14:37:21.700901 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| SurrLoss             | 0.0052665 |
| Entropy              | 0.27096   |
| Perplexity           | 1.3112    |
| AveragePolicyProb[0] | 0.51269   |
| AveragePolicyProb[1] | 0.48731   |
| AverageReturn        | 193.59    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 14.559    |
| AverageEpisodeLength | 193.59    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.559    |
| TotalNEpisodes       | 376       |
| TotalNSamples        | 46543     |
| ExplainedVariance    | 0.27349   |
------------------------------------
[2019-08-24 14:37:22.700323 UTC] Saving snapshot
[2019-08-24 14:37:22.715611 UTC] Starting iteration 24
[2019-08-24 14:37:22.717839 UTC] Start collecting samples
[2019-08-24 14:37:23.028507 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:23.045228 UTC] Computing policy gradient
[2019-08-24 14:37:23.054856 UTC] Updating baseline
[2019-08-24 14:37:23.136606 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| SurrLoss             | 0.0009762 |
| Entropy              | 0.259     |
| Perplexity           | 1.2956    |
| AveragePolicyProb[0] | 0.50239   |
| AveragePolicyProb[1] | 0.49761   |
| AverageReturn        | 195.97    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 11.851    |
| AverageEpisodeLength | 195.97    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.851    |
| TotalNEpisodes       | 387       |
| TotalNSamples        | 48743     |
| ExplainedVariance    | 0.27557   |
------------------------------------
[2019-08-24 14:37:24.137225 UTC] Saving snapshot
[2019-08-24 14:37:24.150062 UTC] Starting iteration 25
[2019-08-24 14:37:24.151215 UTC] Start collecting samples
[2019-08-24 14:37:24.429694 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:24.445786 UTC] Computing policy gradient
[2019-08-24 14:37:24.455602 UTC] Updating baseline
[2019-08-24 14:37:24.536053 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| SurrLoss             | -0.032023 |
| Entropy              | 0.26588   |
| Perplexity           | 1.3046    |
| AveragePolicyProb[0] | 0.49839   |
| AveragePolicyProb[1] | 0.50161   |
| AverageReturn        | 197.67    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 9.0896    |
| AverageEpisodeLength | 197.67    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.0896    |
| TotalNEpisodes       | 395       |
| TotalNSamples        | 50343     |
| ExplainedVariance    | 0.40014   |
------------------------------------
[2019-08-24 14:37:25.540544 UTC] Saving snapshot
[2019-08-24 14:37:25.556611 UTC] Starting iteration 26
[2019-08-24 14:37:25.557813 UTC] Start collecting samples
[2019-08-24 14:37:25.857395 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:25.873922 UTC] Computing policy gradient
[2019-08-24 14:37:25.885363 UTC] Updating baseline
[2019-08-24 14:37:26.029447 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| SurrLoss             | -0.022943 |
| Entropy              | 0.25296   |
| Perplexity           | 1.2878    |
| AveragePolicyProb[0] | 0.49816   |
| AveragePolicyProb[1] | 0.50184   |
| AverageReturn        | 199.88    |
| MinReturn            | 188       |
| MaxReturn            | 200       |
| StdReturn            | 1.194     |
| AverageEpisodeLength | 199.88    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 1.194     |
| TotalNEpisodes       | 404       |
| TotalNSamples        | 52143     |
| ExplainedVariance    | 0.076352  |
------------------------------------
[2019-08-24 14:37:27.022565 UTC] Saving snapshot
[2019-08-24 14:37:27.038721 UTC] Starting iteration 27
[2019-08-24 14:37:27.041558 UTC] Start collecting samples
[2019-08-24 14:37:27.378247 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:27.402225 UTC] Computing policy gradient
[2019-08-24 14:37:27.412529 UTC] Updating baseline
[2019-08-24 14:37:27.528784 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| SurrLoss             | -0.0095373 |
| Entropy              | 0.24784    |
| Perplexity           | 1.2813     |
| AveragePolicyProb[0] | 0.51072    |
| AveragePolicyProb[1] | 0.48928    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 416        |
| TotalNSamples        | 54543      |
| ExplainedVariance    | 0.27292    |
-------------------------------------
[2019-08-24 14:37:28.636470 UTC] Saving snapshot
[2019-08-24 14:37:28.651151 UTC] Starting iteration 28
[2019-08-24 14:37:28.651994 UTC] Start collecting samples
[2019-08-24 14:37:28.941988 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:28.959362 UTC] Computing policy gradient
[2019-08-24 14:37:28.969803 UTC] Updating baseline
[2019-08-24 14:37:29.103091 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| SurrLoss             | -0.015847 |
| Entropy              | 0.24704   |
| Perplexity           | 1.2802    |
| AveragePolicyProb[0] | 0.49776   |
| AveragePolicyProb[1] | 0.50224   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 424       |
| TotalNSamples        | 56143     |
| ExplainedVariance    | 0.47993   |
------------------------------------
[2019-08-24 14:37:30.066116 UTC] Saving snapshot
[2019-08-24 14:37:30.080845 UTC] Starting iteration 29
[2019-08-24 14:37:30.083691 UTC] Start collecting samples
[2019-08-24 14:37:30.395459 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:30.410964 UTC] Computing policy gradient
[2019-08-24 14:37:30.420000 UTC] Updating baseline
[2019-08-24 14:37:30.526540 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| SurrLoss             | 0.0075635 |
| Entropy              | 0.24616   |
| Perplexity           | 1.2791    |
| AveragePolicyProb[0] | 0.50721   |
| AveragePolicyProb[1] | 0.49279   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 58543     |
| ExplainedVariance    | 0.23601   |
------------------------------------
[2019-08-24 14:37:31.507604 UTC] Saving snapshot
[2019-08-24 14:37:31.523209 UTC] Starting iteration 30
[2019-08-24 14:37:31.524628 UTC] Start collecting samples
[2019-08-24 14:37:31.930554 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:31.948719 UTC] Computing policy gradient
[2019-08-24 14:37:31.960225 UTC] Updating baseline
[2019-08-24 14:37:32.093386 UTC] Computing logging information
-----------------------------------
| Iteration            | 30       |
| SurrLoss             | 0.005787 |
| Entropy              | 0.22823  |
| Perplexity           | 1.2564   |
| AveragePolicyProb[0] | 0.49127  |
| AveragePolicyProb[1] | 0.50873  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 445      |
| TotalNSamples        | 60343    |
| ExplainedVariance    | 0.29669  |
-----------------------------------
[2019-08-24 14:37:33.162551 UTC] Saving snapshot
[2019-08-24 14:37:33.180119 UTC] Starting iteration 31
[2019-08-24 14:37:33.181017 UTC] Start collecting samples
[2019-08-24 14:37:33.507620 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:33.525709 UTC] Computing policy gradient
[2019-08-24 14:37:33.539117 UTC] Updating baseline
[2019-08-24 14:37:33.662145 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| SurrLoss             | -0.0040467 |
| Entropy              | 0.23133    |
| Perplexity           | 1.2603     |
| AveragePolicyProb[0] | 0.51008    |
| AveragePolicyProb[1] | 0.48992    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 456        |
| TotalNSamples        | 62543      |
| ExplainedVariance    | 0.28968    |
-------------------------------------
[2019-08-24 14:37:35.073543 UTC] Saving snapshot
[2019-08-24 14:37:35.089009 UTC] Starting iteration 32
[2019-08-24 14:37:35.090841 UTC] Start collecting samples
[2019-08-24 14:37:35.454223 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:35.470776 UTC] Computing policy gradient
[2019-08-24 14:37:35.478571 UTC] Updating baseline
[2019-08-24 14:37:35.559004 UTC] Computing logging information
------------------------------------
| Iteration            | 32        |
| SurrLoss             | 0.0025263 |
| Entropy              | 0.22913   |
| Perplexity           | 1.2575    |
| AveragePolicyProb[0] | 0.50072   |
| AveragePolicyProb[1] | 0.49928   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 467       |
| TotalNSamples        | 64743     |
| ExplainedVariance    | 0.33729   |
------------------------------------
[2019-08-24 14:37:36.477572 UTC] Saving snapshot
[2019-08-24 14:37:36.491505 UTC] Starting iteration 33
[2019-08-24 14:37:36.492857 UTC] Start collecting samples
[2019-08-24 14:37:36.765964 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:36.789756 UTC] Computing policy gradient
[2019-08-24 14:37:36.797496 UTC] Updating baseline
[2019-08-24 14:37:36.903927 UTC] Computing logging information
-------------------------------------
| Iteration            | 33         |
| SurrLoss             | -0.0079408 |
| Entropy              | 0.22302    |
| Perplexity           | 1.2498     |
| AveragePolicyProb[0] | 0.49831    |
| AveragePolicyProb[1] | 0.50169    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 475        |
| TotalNSamples        | 66343      |
| ExplainedVariance    | 0.58555    |
-------------------------------------
[2019-08-24 14:37:38.032108 UTC] Saving snapshot
[2019-08-24 14:37:38.045587 UTC] Starting iteration 34
[2019-08-24 14:37:38.047719 UTC] Start collecting samples
[2019-08-24 14:37:38.295969 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:38.312563 UTC] Computing policy gradient
[2019-08-24 14:37:38.321816 UTC] Updating baseline
[2019-08-24 14:37:38.410034 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| SurrLoss             | 0.0071279 |
| Entropy              | 0.22612   |
| Perplexity           | 1.2537    |
| AveragePolicyProb[0] | 0.4943    |
| AveragePolicyProb[1] | 0.5057    |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 484       |
| TotalNSamples        | 68143     |
| ExplainedVariance    | 0.44132   |
------------------------------------
[2019-08-24 14:37:39.237402 UTC] Saving snapshot
[2019-08-24 14:37:39.250849 UTC] Starting iteration 35
[2019-08-24 14:37:39.252203 UTC] Start collecting samples
[2019-08-24 14:37:39.534102 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:39.551893 UTC] Computing policy gradient
[2019-08-24 14:37:39.561554 UTC] Updating baseline
[2019-08-24 14:37:39.644544 UTC] Computing logging information
-----------------------------------
| Iteration            | 35       |
| SurrLoss             | 0.002427 |
| Entropy              | 0.20857  |
| Perplexity           | 1.2319   |
| AveragePolicyProb[0] | 0.49903  |
| AveragePolicyProb[1] | 0.50097  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 496      |
| TotalNSamples        | 70543    |
| ExplainedVariance    | 0.38423  |
-----------------------------------
[2019-08-24 14:37:40.520311 UTC] Saving snapshot
[2019-08-24 14:37:40.533843 UTC] Starting iteration 36
[2019-08-24 14:37:40.535082 UTC] Start collecting samples
[2019-08-24 14:37:40.779670 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:40.794316 UTC] Computing policy gradient
[2019-08-24 14:37:40.802059 UTC] Updating baseline
[2019-08-24 14:37:40.903934 UTC] Computing logging information
-----------------------------------
| Iteration            | 36       |
| SurrLoss             | 0.002791 |
| Entropy              | 0.2229   |
| Perplexity           | 1.2497   |
| AveragePolicyProb[0] | 0.49724  |
| AveragePolicyProb[1] | 0.50276  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 504      |
| TotalNSamples        | 72143    |
| ExplainedVariance    | 0.49591  |
-----------------------------------
[2019-08-24 14:37:41.754534 UTC] Saving snapshot
[2019-08-24 14:37:41.767526 UTC] Starting iteration 37
[2019-08-24 14:37:41.769064 UTC] Start collecting samples
[2019-08-24 14:37:42.060495 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:42.077499 UTC] Computing policy gradient
[2019-08-24 14:37:42.086834 UTC] Updating baseline
[2019-08-24 14:37:42.171069 UTC] Computing logging information
-----------------------------------
| Iteration            | 37       |
| SurrLoss             | 0.008625 |
| Entropy              | 0.20529  |
| Perplexity           | 1.2279   |
| AveragePolicyProb[0] | 0.4921   |
| AveragePolicyProb[1] | 0.5079   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 516      |
| TotalNSamples        | 74543    |
| ExplainedVariance    | 0.25107  |
-----------------------------------
[2019-08-24 14:37:43.027047 UTC] Saving snapshot
[2019-08-24 14:37:43.040068 UTC] Starting iteration 38
[2019-08-24 14:37:43.041251 UTC] Start collecting samples
[2019-08-24 14:37:43.287455 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:43.304655 UTC] Computing policy gradient
[2019-08-24 14:37:43.314637 UTC] Updating baseline
[2019-08-24 14:37:43.424568 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| SurrLoss             | -0.010286 |
| Entropy              | 0.21922   |
| Perplexity           | 1.2451    |
| AveragePolicyProb[0] | 0.50004   |
| AveragePolicyProb[1] | 0.49996   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 525       |
| TotalNSamples        | 76343     |
| ExplainedVariance    | 0.44444   |
------------------------------------
[2019-08-24 14:37:44.278921 UTC] Saving snapshot
[2019-08-24 14:37:44.291789 UTC] Starting iteration 39
[2019-08-24 14:37:44.292624 UTC] Start collecting samples
[2019-08-24 14:37:44.569385 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:44.586311 UTC] Computing policy gradient
[2019-08-24 14:37:44.595732 UTC] Updating baseline
[2019-08-24 14:37:44.703077 UTC] Computing logging information
-----------------------------------
| Iteration            | 39       |
| SurrLoss             | 0.013774 |
| Entropy              | 0.21234  |
| Perplexity           | 1.2366   |
| AveragePolicyProb[0] | 0.50515  |
| AveragePolicyProb[1] | 0.49485  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 536      |
| TotalNSamples        | 78543    |
| ExplainedVariance    | 0.32796  |
-----------------------------------
[2019-08-24 14:37:45.676538 UTC] Saving snapshot
[2019-08-24 14:37:45.689259 UTC] Starting iteration 40
[2019-08-24 14:37:45.690383 UTC] Start collecting samples
[2019-08-24 14:37:45.954513 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:45.970514 UTC] Computing policy gradient
[2019-08-24 14:37:45.978548 UTC] Updating baseline
[2019-08-24 14:37:46.078768 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| SurrLoss             | -0.0053414 |
| Entropy              | 0.21783    |
| Perplexity           | 1.2434     |
| AveragePolicyProb[0] | 0.50383    |
| AveragePolicyProb[1] | 0.49617    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 547        |
| TotalNSamples        | 80743      |
| ExplainedVariance    | 0.56323    |
-------------------------------------
[2019-08-24 14:37:46.881144 UTC] Saving snapshot
[2019-08-24 14:37:46.892783 UTC] Starting iteration 41
[2019-08-24 14:37:46.893743 UTC] Start collecting samples
[2019-08-24 14:37:47.126178 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:47.147295 UTC] Computing policy gradient
[2019-08-24 14:37:47.158911 UTC] Updating baseline
[2019-08-24 14:37:47.259542 UTC] Computing logging information
-----------------------------------
| Iteration            | 41       |
| SurrLoss             | 0.013858 |
| Entropy              | 0.23469  |
| Perplexity           | 1.2645   |
| AveragePolicyProb[0] | 0.4953   |
| AveragePolicyProb[1] | 0.5047   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 555      |
| TotalNSamples        | 82343    |
| ExplainedVariance    | 0.53998  |
-----------------------------------
[2019-08-24 14:37:48.176782 UTC] Saving snapshot
[2019-08-24 14:37:48.190656 UTC] Starting iteration 42
[2019-08-24 14:37:48.191594 UTC] Start collecting samples
[2019-08-24 14:37:48.489634 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:48.507638 UTC] Computing policy gradient
[2019-08-24 14:37:48.516827 UTC] Updating baseline
[2019-08-24 14:37:48.605567 UTC] Computing logging information
-----------------------------------
| Iteration            | 42       |
| SurrLoss             | 0.004802 |
| Entropy              | 0.22141  |
| Perplexity           | 1.2478   |
| AveragePolicyProb[0] | 0.49836  |
| AveragePolicyProb[1] | 0.50164  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 564      |
| TotalNSamples        | 84143    |
| ExplainedVariance    | 0.49174  |
-----------------------------------
[2019-08-24 14:37:49.412840 UTC] Saving snapshot
[2019-08-24 14:37:49.426705 UTC] Starting iteration 43
[2019-08-24 14:37:49.428787 UTC] Start collecting samples
[2019-08-24 14:37:49.699528 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:49.719194 UTC] Computing policy gradient
[2019-08-24 14:37:49.728269 UTC] Updating baseline
[2019-08-24 14:37:49.823509 UTC] Computing logging information
-----------------------------------
| Iteration            | 43       |
| SurrLoss             | 0.023474 |
| Entropy              | 0.22058  |
| Perplexity           | 1.2468   |
| AveragePolicyProb[0] | 0.50365  |
| AveragePolicyProb[1] | 0.49636  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 576      |
| TotalNSamples        | 86543    |
| ExplainedVariance    | 0.39104  |
-----------------------------------
[2019-08-24 14:37:50.682914 UTC] Saving snapshot
[2019-08-24 14:37:50.696492 UTC] Starting iteration 44
[2019-08-24 14:37:50.697534 UTC] Start collecting samples
[2019-08-24 14:37:50.948197 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:50.963462 UTC] Computing policy gradient
[2019-08-24 14:37:50.970626 UTC] Updating baseline
[2019-08-24 14:37:51.077999 UTC] Computing logging information
------------------------------------
| Iteration            | 44        |
| SurrLoss             | 0.0061807 |
| Entropy              | 0.22472   |
| Perplexity           | 1.252     |
| AveragePolicyProb[0] | 0.50292   |
| AveragePolicyProb[1] | 0.49708   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 584       |
| TotalNSamples        | 88143     |
| ExplainedVariance    | 0.48984   |
------------------------------------
[2019-08-24 14:37:51.939431 UTC] Saving snapshot
[2019-08-24 14:37:51.952943 UTC] Starting iteration 45
[2019-08-24 14:37:51.954581 UTC] Start collecting samples
[2019-08-24 14:37:52.232893 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:52.252003 UTC] Computing policy gradient
[2019-08-24 14:37:52.261561 UTC] Updating baseline
[2019-08-24 14:37:52.357551 UTC] Computing logging information
------------------------------------
| Iteration            | 45        |
| SurrLoss             | 0.0047472 |
| Entropy              | 0.24152   |
| Perplexity           | 1.2732    |
| AveragePolicyProb[0] | 0.49427   |
| AveragePolicyProb[1] | 0.50573   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 596       |
| TotalNSamples        | 90543     |
| ExplainedVariance    | 0.28464   |
------------------------------------
[2019-08-24 14:37:53.259129 UTC] Saving snapshot
[2019-08-24 14:37:53.272472 UTC] Starting iteration 46
[2019-08-24 14:37:53.273892 UTC] Start collecting samples
[2019-08-24 14:37:53.540952 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:53.558101 UTC] Computing policy gradient
[2019-08-24 14:37:53.567866 UTC] Updating baseline
[2019-08-24 14:37:53.665930 UTC] Computing logging information
------------------------------------
| Iteration            | 46        |
| SurrLoss             | -0.016625 |
| Entropy              | 0.25168   |
| Perplexity           | 1.2862    |
| AveragePolicyProb[0] | 0.50086   |
| AveragePolicyProb[1] | 0.49914   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 605       |
| TotalNSamples        | 92343     |
| ExplainedVariance    | 0.25357   |
------------------------------------
[2019-08-24 14:37:54.523582 UTC] Saving snapshot
[2019-08-24 14:37:54.540058 UTC] Starting iteration 47
[2019-08-24 14:37:54.542709 UTC] Start collecting samples
[2019-08-24 14:37:54.887757 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:54.906128 UTC] Computing policy gradient
[2019-08-24 14:37:54.918391 UTC] Updating baseline
[2019-08-24 14:37:55.019083 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| SurrLoss             | -0.015742 |
| Entropy              | 0.25732   |
| Perplexity           | 1.2935    |
| AveragePolicyProb[0] | 0.49697   |
| AveragePolicyProb[1] | 0.50303   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 616       |
| TotalNSamples        | 94543     |
| ExplainedVariance    | 0.19877   |
------------------------------------
[2019-08-24 14:37:56.280722 UTC] Saving snapshot
[2019-08-24 14:37:56.296181 UTC] Starting iteration 48
[2019-08-24 14:37:56.297847 UTC] Start collecting samples
[2019-08-24 14:37:56.573994 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:56.590984 UTC] Computing policy gradient
[2019-08-24 14:37:56.600516 UTC] Updating baseline
[2019-08-24 14:37:56.698133 UTC] Computing logging information
--------------------------------------
| Iteration            | 48          |
| SurrLoss             | -0.00068493 |
| Entropy              | 0.25687     |
| Perplexity           | 1.2929      |
| AveragePolicyProb[0] | 0.50604     |
| AveragePolicyProb[1] | 0.49396     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 627         |
| TotalNSamples        | 96743       |
| ExplainedVariance    | 0.32934     |
--------------------------------------
[2019-08-24 14:37:57.669058 UTC] Saving snapshot
[2019-08-24 14:37:57.681838 UTC] Starting iteration 49
[2019-08-24 14:37:57.683400 UTC] Start collecting samples
[2019-08-24 14:37:57.928688 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:57.946137 UTC] Computing policy gradient
[2019-08-24 14:37:57.955393 UTC] Updating baseline
[2019-08-24 14:37:58.068002 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| SurrLoss             | 0.0078327 |
| Entropy              | 0.27114   |
| Perplexity           | 1.3115    |
| AveragePolicyProb[0] | 0.49124   |
| AveragePolicyProb[1] | 0.50876   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 635       |
| TotalNSamples        | 98343     |
| ExplainedVariance    | 0.55039   |
------------------------------------
[2019-08-24 14:37:58.984713 UTC] Saving snapshot
[2019-08-24 14:37:58.998005 UTC] Starting iteration 50
[2019-08-24 14:37:58.999364 UTC] Start collecting samples
[2019-08-24 14:37:59.288324 UTC] Computing input variables for policy optimization
[2019-08-24 14:37:59.308515 UTC] Computing policy gradient
[2019-08-24 14:37:59.316782 UTC] Updating baseline
[2019-08-24 14:37:59.415736 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| SurrLoss             | -0.014792  |
| Entropy              | 0.28492    |
| Perplexity           | 1.3297     |
| AveragePolicyProb[0] | 0.50422    |
| AveragePolicyProb[1] | 0.49578    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 644        |
| TotalNSamples        | 1.0014e+05 |
| ExplainedVariance    | 0.35474    |
-------------------------------------
[2019-08-24 14:38:00.320400 UTC] Saving snapshot
[2019-08-24 14:38:00.336521 UTC] Starting iteration 51
[2019-08-24 14:38:00.337446 UTC] Start collecting samples
[2019-08-24 14:38:00.658082 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:00.674873 UTC] Computing policy gradient
[2019-08-24 14:38:00.684612 UTC] Updating baseline
[2019-08-24 14:38:00.773012 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| SurrLoss             | 0.0054623  |
| Entropy              | 0.28651    |
| Perplexity           | 1.3318     |
| AveragePolicyProb[0] | 0.50818    |
| AveragePolicyProb[1] | 0.49182    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 656        |
| TotalNSamples        | 1.0254e+05 |
| ExplainedVariance    | 0.27268    |
-------------------------------------
[2019-08-24 14:38:01.719506 UTC] Saving snapshot
[2019-08-24 14:38:01.732203 UTC] Starting iteration 52
[2019-08-24 14:38:01.733434 UTC] Start collecting samples
[2019-08-24 14:38:02.038011 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:02.058258 UTC] Computing policy gradient
[2019-08-24 14:38:02.067722 UTC] Updating baseline
[2019-08-24 14:38:02.160154 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| SurrLoss             | -0.029588  |
| Entropy              | 0.30086    |
| Perplexity           | 1.351      |
| AveragePolicyProb[0] | 0.50746    |
| AveragePolicyProb[1] | 0.49254    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 664        |
| TotalNSamples        | 1.0414e+05 |
| ExplainedVariance    | 0.17376    |
-------------------------------------
[2019-08-24 14:38:02.988069 UTC] Saving snapshot
[2019-08-24 14:38:03.001500 UTC] Starting iteration 53
[2019-08-24 14:38:03.003240 UTC] Start collecting samples
[2019-08-24 14:38:03.338473 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:03.371486 UTC] Computing policy gradient
[2019-08-24 14:38:03.383158 UTC] Updating baseline
[2019-08-24 14:38:03.466997 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| SurrLoss             | -0.015965  |
| Entropy              | 0.29288    |
| Perplexity           | 1.3403     |
| AveragePolicyProb[0] | 0.4939     |
| AveragePolicyProb[1] | 0.5061     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 676        |
| TotalNSamples        | 1.0654e+05 |
| ExplainedVariance    | 0.19981    |
-------------------------------------
[2019-08-24 14:38:04.334754 UTC] Saving snapshot
[2019-08-24 14:38:04.347593 UTC] Starting iteration 54
[2019-08-24 14:38:04.348791 UTC] Start collecting samples
[2019-08-24 14:38:04.623518 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:04.641351 UTC] Computing policy gradient
[2019-08-24 14:38:04.650701 UTC] Updating baseline
[2019-08-24 14:38:04.753763 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| SurrLoss             | -0.0065934 |
| Entropy              | 0.28847    |
| Perplexity           | 1.3344     |
| AveragePolicyProb[0] | 0.48555    |
| AveragePolicyProb[1] | 0.51445    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 685        |
| TotalNSamples        | 1.0834e+05 |
| ExplainedVariance    | 0.33892    |
-------------------------------------
[2019-08-24 14:38:05.634685 UTC] Saving snapshot
[2019-08-24 14:38:05.651179 UTC] Starting iteration 55
[2019-08-24 14:38:05.653258 UTC] Start collecting samples
[2019-08-24 14:38:05.941449 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:05.957378 UTC] Computing policy gradient
[2019-08-24 14:38:05.965489 UTC] Updating baseline
[2019-08-24 14:38:06.061408 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| SurrLoss             | 0.0058793  |
| Entropy              | 0.29937    |
| Perplexity           | 1.349      |
| AveragePolicyProb[0] | 0.51262    |
| AveragePolicyProb[1] | 0.48738    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 696        |
| TotalNSamples        | 1.1054e+05 |
| ExplainedVariance    | 0.092993   |
-------------------------------------
[2019-08-24 14:38:06.958246 UTC] Saving snapshot
[2019-08-24 14:38:06.973316 UTC] Starting iteration 56
[2019-08-24 14:38:06.975748 UTC] Start collecting samples
[2019-08-24 14:38:07.304042 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:07.321892 UTC] Computing policy gradient
[2019-08-24 14:38:07.332441 UTC] Updating baseline
[2019-08-24 14:38:07.438278 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| SurrLoss             | -0.0049614 |
| Entropy              | 0.29723    |
| Perplexity           | 1.3461     |
| AveragePolicyProb[0] | 0.48553    |
| AveragePolicyProb[1] | 0.51447    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 707        |
| TotalNSamples        | 1.1274e+05 |
| ExplainedVariance    | 0.31305    |
-------------------------------------
[2019-08-24 14:38:08.342516 UTC] Saving snapshot
[2019-08-24 14:38:08.358805 UTC] Starting iteration 57
[2019-08-24 14:38:08.359697 UTC] Start collecting samples
[2019-08-24 14:38:08.634701 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:08.649003 UTC] Computing policy gradient
[2019-08-24 14:38:08.656716 UTC] Updating baseline
[2019-08-24 14:38:08.758522 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| SurrLoss             | -0.0026451 |
| Entropy              | 0.3004     |
| Perplexity           | 1.3504     |
| AveragePolicyProb[0] | 0.50657    |
| AveragePolicyProb[1] | 0.49343    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 715        |
| TotalNSamples        | 1.1434e+05 |
| ExplainedVariance    | 0.29851    |
-------------------------------------
[2019-08-24 14:38:09.671569 UTC] Saving snapshot
[2019-08-24 14:38:09.684266 UTC] Starting iteration 58
[2019-08-24 14:38:09.685271 UTC] Start collecting samples
[2019-08-24 14:38:09.966140 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:09.981169 UTC] Computing policy gradient
[2019-08-24 14:38:09.988182 UTC] Updating baseline
[2019-08-24 14:38:10.082265 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| SurrLoss             | -0.033169  |
| Entropy              | 0.30783    |
| Perplexity           | 1.3605     |
| AveragePolicyProb[0] | 0.50619    |
| AveragePolicyProb[1] | 0.49381    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 724        |
| TotalNSamples        | 1.1614e+05 |
| ExplainedVariance    | 0.37634    |
-------------------------------------
[2019-08-24 14:38:10.886533 UTC] Saving snapshot
[2019-08-24 14:38:10.899552 UTC] Starting iteration 59
[2019-08-24 14:38:10.900747 UTC] Start collecting samples
[2019-08-24 14:38:11.471800 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:11.497335 UTC] Computing policy gradient
[2019-08-24 14:38:11.503942 UTC] Updating baseline
[2019-08-24 14:38:11.612917 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| SurrLoss             | 0.01208    |
| Entropy              | 0.29629    |
| Perplexity           | 1.3449     |
| AveragePolicyProb[0] | 0.50863    |
| AveragePolicyProb[1] | 0.49137    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 736        |
| TotalNSamples        | 1.1854e+05 |
| ExplainedVariance    | 0.44955    |
-------------------------------------
[2019-08-24 14:38:12.496684 UTC] Saving snapshot
[2019-08-24 14:38:12.510753 UTC] Starting iteration 60
[2019-08-24 14:38:12.511746 UTC] Start collecting samples
[2019-08-24 14:38:12.761055 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:12.776697 UTC] Computing policy gradient
[2019-08-24 14:38:12.786276 UTC] Updating baseline
[2019-08-24 14:38:12.863253 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| SurrLoss             | 0.011792   |
| Entropy              | 0.30345    |
| Perplexity           | 1.3545     |
| AveragePolicyProb[0] | 0.49837    |
| AveragePolicyProb[1] | 0.50163    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 744        |
| TotalNSamples        | 1.2014e+05 |
| ExplainedVariance    | 0.67098    |
-------------------------------------
[2019-08-24 14:38:13.818200 UTC] Saving snapshot
[2019-08-24 14:38:13.832255 UTC] Starting iteration 61
[2019-08-24 14:38:13.833499 UTC] Start collecting samples
[2019-08-24 14:38:14.173764 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:14.191944 UTC] Computing policy gradient
[2019-08-24 14:38:14.199691 UTC] Updating baseline
[2019-08-24 14:38:14.297249 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| SurrLoss             | -0.004161  |
| Entropy              | 0.32662    |
| Perplexity           | 1.3863     |
| AveragePolicyProb[0] | 0.51158    |
| AveragePolicyProb[1] | 0.48842    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 756        |
| TotalNSamples        | 1.2254e+05 |
| ExplainedVariance    | 0.74187    |
-------------------------------------
[2019-08-24 14:38:15.481561 UTC] Saving snapshot
[2019-08-24 14:38:15.497093 UTC] Starting iteration 62
[2019-08-24 14:38:15.498844 UTC] Start collecting samples
[2019-08-24 14:38:15.782315 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:15.798430 UTC] Computing policy gradient
[2019-08-24 14:38:15.805788 UTC] Updating baseline
[2019-08-24 14:38:15.902589 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| SurrLoss             | -0.010567  |
| Entropy              | 0.33196    |
| Perplexity           | 1.3937     |
| AveragePolicyProb[0] | 0.5016     |
| AveragePolicyProb[1] | 0.4984     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 765        |
| TotalNSamples        | 1.2434e+05 |
| ExplainedVariance    | 0.73904    |
-------------------------------------
[2019-08-24 14:38:17.095803 UTC] Saving snapshot
[2019-08-24 14:38:17.111328 UTC] Starting iteration 63
[2019-08-24 14:38:17.112305 UTC] Start collecting samples
[2019-08-24 14:38:17.455492 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:17.478294 UTC] Computing policy gradient
[2019-08-24 14:38:17.487872 UTC] Updating baseline
[2019-08-24 14:38:17.592238 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| SurrLoss             | -0.0084199 |
| Entropy              | 0.32799    |
| Perplexity           | 1.3882     |
| AveragePolicyProb[0] | 0.50785    |
| AveragePolicyProb[1] | 0.49215    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 776        |
| TotalNSamples        | 1.2654e+05 |
| ExplainedVariance    | 0.60613    |
-------------------------------------
[2019-08-24 14:38:18.518743 UTC] Saving snapshot
[2019-08-24 14:38:18.531779 UTC] Starting iteration 64
[2019-08-24 14:38:18.534311 UTC] Start collecting samples
[2019-08-24 14:38:18.858618 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:18.881126 UTC] Computing policy gradient
[2019-08-24 14:38:18.892321 UTC] Updating baseline
[2019-08-24 14:38:19.016751 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| SurrLoss             | -0.015793  |
| Entropy              | 0.3336     |
| Perplexity           | 1.396      |
| AveragePolicyProb[0] | 0.50966    |
| AveragePolicyProb[1] | 0.49034    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 787        |
| TotalNSamples        | 1.2874e+05 |
| ExplainedVariance    | 0.58451    |
-------------------------------------
[2019-08-24 14:38:20.111385 UTC] Saving snapshot
[2019-08-24 14:38:20.124699 UTC] Starting iteration 65
[2019-08-24 14:38:20.126209 UTC] Start collecting samples
[2019-08-24 14:38:20.371625 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:20.390319 UTC] Computing policy gradient
[2019-08-24 14:38:20.402324 UTC] Updating baseline
[2019-08-24 14:38:20.509137 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| SurrLoss             | 0.0030379  |
| Entropy              | 0.33822    |
| Perplexity           | 1.4025     |
| AveragePolicyProb[0] | 0.48834    |
| AveragePolicyProb[1] | 0.51166    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 795        |
| TotalNSamples        | 1.3034e+05 |
| ExplainedVariance    | 0.43733    |
-------------------------------------
[2019-08-24 14:38:21.348059 UTC] Saving snapshot
[2019-08-24 14:38:21.361141 UTC] Starting iteration 66
[2019-08-24 14:38:21.362281 UTC] Start collecting samples
[2019-08-24 14:38:21.612872 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:21.628188 UTC] Computing policy gradient
[2019-08-24 14:38:21.637133 UTC] Updating baseline
[2019-08-24 14:38:21.731502 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| SurrLoss             | 0.0055713  |
| Entropy              | 0.34191    |
| Perplexity           | 1.4076     |
| AveragePolicyProb[0] | 0.50055    |
| AveragePolicyProb[1] | 0.49945    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 804        |
| TotalNSamples        | 1.3214e+05 |
| ExplainedVariance    | 0.44698    |
-------------------------------------
[2019-08-24 14:38:22.563424 UTC] Saving snapshot
[2019-08-24 14:38:22.576305 UTC] Starting iteration 67
[2019-08-24 14:38:22.577883 UTC] Start collecting samples
[2019-08-24 14:38:22.880360 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:22.898023 UTC] Computing policy gradient
[2019-08-24 14:38:22.905496 UTC] Updating baseline
[2019-08-24 14:38:23.018579 UTC] Computing logging information
--------------------------------------
| Iteration            | 67          |
| SurrLoss             | -0.00046859 |
| Entropy              | 0.35142     |
| Perplexity           | 1.4211      |
| AveragePolicyProb[0] | 0.50166     |
| AveragePolicyProb[1] | 0.49834     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 816         |
| TotalNSamples        | 1.3454e+05  |
| ExplainedVariance    | 0.22474     |
--------------------------------------
[2019-08-24 14:38:23.821374 UTC] Saving snapshot
[2019-08-24 14:38:23.834643 UTC] Starting iteration 68
[2019-08-24 14:38:23.835853 UTC] Start collecting samples
[2019-08-24 14:38:24.076635 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:24.091819 UTC] Computing policy gradient
[2019-08-24 14:38:24.104100 UTC] Updating baseline
[2019-08-24 14:38:24.216091 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| SurrLoss             | 0.0061919  |
| Entropy              | 0.37468    |
| Perplexity           | 1.4545     |
| AveragePolicyProb[0] | 0.5017     |
| AveragePolicyProb[1] | 0.4983     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 824        |
| TotalNSamples        | 1.3614e+05 |
| ExplainedVariance    | 0.2511     |
-------------------------------------
[2019-08-24 14:38:25.102244 UTC] Saving snapshot
[2019-08-24 14:38:25.115003 UTC] Starting iteration 69
[2019-08-24 14:38:25.116607 UTC] Start collecting samples
[2019-08-24 14:38:25.446565 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:25.463734 UTC] Computing policy gradient
[2019-08-24 14:38:25.474353 UTC] Updating baseline
[2019-08-24 14:38:25.573177 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| SurrLoss             | 0.016379   |
| Entropy              | 0.39508    |
| Perplexity           | 1.4845     |
| AveragePolicyProb[0] | 0.4992     |
| AveragePolicyProb[1] | 0.5008     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 836        |
| TotalNSamples        | 1.3854e+05 |
| ExplainedVariance    | 0.013234   |
-------------------------------------
[2019-08-24 14:38:26.464327 UTC] Saving snapshot
[2019-08-24 14:38:26.475583 UTC] Starting iteration 70
[2019-08-24 14:38:26.476512 UTC] Start collecting samples
[2019-08-24 14:38:26.779233 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:26.795870 UTC] Computing policy gradient
[2019-08-24 14:38:26.802523 UTC] Updating baseline
[2019-08-24 14:38:26.921875 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| SurrLoss             | 0.008201   |
| Entropy              | 0.43069    |
| Perplexity           | 1.5383     |
| AveragePolicyProb[0] | 0.50651    |
| AveragePolicyProb[1] | 0.49349    |
| AverageReturn        | 198.67     |
| MinReturn            | 101        |
| MaxReturn            | 200        |
| StdReturn            | 10.383     |
| AverageEpisodeLength | 198.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 10.383     |
| TotalNEpisodes       | 846        |
| TotalNSamples        | 1.4041e+05 |
| ExplainedVariance    | 0.39768    |
-------------------------------------
[2019-08-24 14:38:28.090602 UTC] Saving snapshot
[2019-08-24 14:38:28.106208 UTC] Starting iteration 71
[2019-08-24 14:38:28.107176 UTC] Start collecting samples
[2019-08-24 14:38:28.775604 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:28.807150 UTC] Computing policy gradient
[2019-08-24 14:38:28.818035 UTC] Updating baseline
[2019-08-24 14:38:28.922631 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| SurrLoss             | 0.045265   |
| Entropy              | 0.37878    |
| Perplexity           | 1.4605     |
| AveragePolicyProb[0] | 0.54676    |
| AveragePolicyProb[1] | 0.45324    |
| AverageReturn        | 143.07     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 77.978     |
| AverageEpisodeLength | 143.07     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 77.978     |
| TotalNEpisodes       | 889        |
| TotalNSamples        | 1.4345e+05 |
| ExplainedVariance    | -0.22732   |
-------------------------------------
[2019-08-24 14:38:29.999683 UTC] Saving snapshot
[2019-08-24 14:38:30.015851 UTC] Starting iteration 72
[2019-08-24 14:38:30.016922 UTC] Start collecting samples
[2019-08-24 14:38:30.274477 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:30.289421 UTC] Computing policy gradient
[2019-08-24 14:38:30.297105 UTC] Updating baseline
[2019-08-24 14:38:30.377892 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| SurrLoss             | 0.032453   |
| Entropy              | 0.38764    |
| Perplexity           | 1.4735     |
| AveragePolicyProb[0] | 0.49079    |
| AveragePolicyProb[1] | 0.50921    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 896        |
| TotalNSamples        | 1.4405e+05 |
| ExplainedVariance    | 0.46846    |
-------------------------------------
[2019-08-24 14:38:31.310900 UTC] Saving snapshot
[2019-08-24 14:38:31.322992 UTC] Starting iteration 73
[2019-08-24 14:38:31.324262 UTC] Start collecting samples
[2019-08-24 14:38:31.611858 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:31.629936 UTC] Computing policy gradient
[2019-08-24 14:38:31.637886 UTC] Updating baseline
[2019-08-24 14:38:31.734193 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| SurrLoss             | -0.0044491 |
| Entropy              | 0.35521    |
| Perplexity           | 1.4265     |
| AveragePolicyProb[0] | 0.49371    |
| AveragePolicyProb[1] | 0.50629    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 909        |
| TotalNSamples        | 1.4665e+05 |
| ExplainedVariance    | 0.73063    |
-------------------------------------
[2019-08-24 14:38:32.644301 UTC] Saving snapshot
[2019-08-24 14:38:32.657726 UTC] Starting iteration 74
[2019-08-24 14:38:32.658773 UTC] Start collecting samples
[2019-08-24 14:38:32.943562 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:32.957650 UTC] Computing policy gradient
[2019-08-24 14:38:32.964830 UTC] Updating baseline
[2019-08-24 14:38:33.058747 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| SurrLoss             | -0.0023291 |
| Entropy              | 0.32163    |
| Perplexity           | 1.3794     |
| AveragePolicyProb[0] | 0.49411    |
| AveragePolicyProb[1] | 0.50589    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 916        |
| TotalNSamples        | 1.4805e+05 |
| ExplainedVariance    | 0.8342     |
-------------------------------------
[2019-08-24 14:38:33.772244 UTC] Saving snapshot
[2019-08-24 14:38:33.782719 UTC] Starting iteration 75
[2019-08-24 14:38:33.783419 UTC] Start collecting samples
[2019-08-24 14:38:34.118696 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:34.133833 UTC] Computing policy gradient
[2019-08-24 14:38:34.140679 UTC] Updating baseline
[2019-08-24 14:38:34.218093 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| SurrLoss             | -0.010632  |
| Entropy              | 0.27663    |
| Perplexity           | 1.3187     |
| AveragePolicyProb[0] | 0.51146    |
| AveragePolicyProb[1] | 0.48854    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 927        |
| TotalNSamples        | 1.5025e+05 |
| ExplainedVariance    | 0.8223     |
-------------------------------------
[2019-08-24 14:38:35.133143 UTC] Saving snapshot
[2019-08-24 14:38:35.146407 UTC] Starting iteration 76
[2019-08-24 14:38:35.147317 UTC] Start collecting samples
[2019-08-24 14:38:35.454213 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:35.471908 UTC] Computing policy gradient
[2019-08-24 14:38:35.481543 UTC] Updating baseline
[2019-08-24 14:38:35.579732 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| SurrLoss             | -0.0065858 |
| Entropy              | 0.25984    |
| Perplexity           | 1.2967     |
| AveragePolicyProb[0] | 0.49691    |
| AveragePolicyProb[1] | 0.50309    |
| AverageReturn        | 136.02     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.262     |
| AverageEpisodeLength | 136.02     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.262     |
| TotalNEpisodes       | 941        |
| TotalNSamples        | 1.5305e+05 |
| ExplainedVariance    | 0.75918    |
-------------------------------------
[2019-08-24 14:38:36.512211 UTC] Saving snapshot
[2019-08-24 14:38:36.525069 UTC] Starting iteration 77
[2019-08-24 14:38:36.526476 UTC] Start collecting samples
[2019-08-24 14:38:36.787133 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:36.799985 UTC] Computing policy gradient
[2019-08-24 14:38:36.808533 UTC] Updating baseline
[2019-08-24 14:38:36.908215 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| SurrLoss             | -0.0041449 |
| Entropy              | 0.23782    |
| Perplexity           | 1.2685     |
| AveragePolicyProb[0] | 0.50053    |
| AveragePolicyProb[1] | 0.49947    |
| AverageReturn        | 136.36     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.462     |
| AverageEpisodeLength | 136.36     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.462     |
| TotalNEpisodes       | 945        |
| TotalNSamples        | 1.5385e+05 |
| ExplainedVariance    | 0.66709    |
-------------------------------------
[2019-08-24 14:38:37.634639 UTC] Saving snapshot
[2019-08-24 14:38:37.645215 UTC] Starting iteration 78
[2019-08-24 14:38:37.645944 UTC] Start collecting samples
[2019-08-24 14:38:37.955684 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:37.972707 UTC] Computing policy gradient
[2019-08-24 14:38:37.981712 UTC] Updating baseline
[2019-08-24 14:38:38.080660 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| SurrLoss             | 0.0011773  |
| Entropy              | 0.22558    |
| Perplexity           | 1.2531     |
| AveragePolicyProb[0] | 0.49331    |
| AveragePolicyProb[1] | 0.50669    |
| AverageReturn        | 146.62     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 76.754     |
| AverageEpisodeLength | 146.62     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 76.754     |
| TotalNEpisodes       | 958        |
| TotalNSamples        | 1.5645e+05 |
| ExplainedVariance    | 0.43945    |
-------------------------------------
[2019-08-24 14:38:38.939980 UTC] Saving snapshot
[2019-08-24 14:38:38.952764 UTC] Starting iteration 79
[2019-08-24 14:38:38.953899 UTC] Start collecting samples
[2019-08-24 14:38:39.257855 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:39.276912 UTC] Computing policy gradient
[2019-08-24 14:38:39.285442 UTC] Updating baseline
[2019-08-24 14:38:39.382054 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| SurrLoss             | -0.0034739 |
| Entropy              | 0.23803    |
| Perplexity           | 1.2687     |
| AveragePolicyProb[0] | 0.50235    |
| AveragePolicyProb[1] | 0.49765    |
| AverageReturn        | 164.7      |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 66.74      |
| AverageEpisodeLength | 164.7      |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 66.74      |
| TotalNEpisodes       | 971        |
| TotalNSamples        | 1.5905e+05 |
| ExplainedVariance    | 0.2124     |
-------------------------------------
[2019-08-24 14:38:40.137349 UTC] Saving snapshot
[2019-08-24 14:38:40.151619 UTC] Starting iteration 80
[2019-08-24 14:38:40.152413 UTC] Start collecting samples
[2019-08-24 14:38:40.400375 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:40.414517 UTC] Computing policy gradient
[2019-08-24 14:38:40.420677 UTC] Updating baseline
[2019-08-24 14:38:40.511443 UTC] Computing logging information
--------------------------------------
| Iteration            | 80          |
| SurrLoss             | -0.00021024 |
| Entropy              | 0.20728     |
| Perplexity           | 1.2303      |
| AveragePolicyProb[0] | 0.50687     |
| AveragePolicyProb[1] | 0.49313     |
| AverageReturn        | 172.85      |
| MinReturn            | 10          |
| MaxReturn            | 200         |
| StdReturn            | 59.749      |
| AverageEpisodeLength | 172.85      |
| MinEpisodeLength     | 10          |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 59.749      |
| TotalNEpisodes       | 976         |
| TotalNSamples        | 1.6005e+05  |
| ExplainedVariance    | 0.14898     |
--------------------------------------
[2019-08-24 14:38:41.241567 UTC] Saving snapshot
[2019-08-24 14:38:41.253833 UTC] Starting iteration 81
[2019-08-24 14:38:41.254996 UTC] Start collecting samples
[2019-08-24 14:38:41.585317 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:41.602767 UTC] Computing policy gradient
[2019-08-24 14:38:41.612052 UTC] Updating baseline
[2019-08-24 14:38:41.719596 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| SurrLoss             | -0.0025101 |
| Entropy              | 0.19662    |
| Perplexity           | 1.2173     |
| AveragePolicyProb[0] | 0.49724    |
| AveragePolicyProb[1] | 0.50276    |
| AverageReturn        | 191.96     |
| MinReturn            | 12         |
| MaxReturn            | 200        |
| StdReturn            | 32.978     |
| AverageEpisodeLength | 191.96     |
| MinEpisodeLength     | 12         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 32.978     |
| TotalNEpisodes       | 989        |
| TotalNSamples        | 1.6265e+05 |
| ExplainedVariance    | -0.11324   |
-------------------------------------
[2019-08-24 14:38:42.403590 UTC] Saving snapshot
[2019-08-24 14:38:42.416371 UTC] Starting iteration 82
[2019-08-24 14:38:42.418404 UTC] Start collecting samples
[2019-08-24 14:38:42.695831 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:42.713331 UTC] Computing policy gradient
[2019-08-24 14:38:42.721011 UTC] Updating baseline
[2019-08-24 14:38:42.830313 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| SurrLoss             | -0.0071702 |
| Entropy              | 0.21372    |
| Perplexity           | 1.2383     |
| AveragePolicyProb[0] | 0.49627    |
| AveragePolicyProb[1] | 0.50373    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 996        |
| TotalNSamples        | 1.6405e+05 |
| ExplainedVariance    | 0.49696    |
-------------------------------------
[2019-08-24 14:38:43.641592 UTC] Saving snapshot
[2019-08-24 14:38:43.652592 UTC] Starting iteration 83
[2019-08-24 14:38:43.654686 UTC] Start collecting samples
[2019-08-24 14:38:43.934760 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:43.957138 UTC] Computing policy gradient
[2019-08-24 14:38:43.969002 UTC] Updating baseline
[2019-08-24 14:38:44.132643 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| SurrLoss             | 0.017254   |
| Entropy              | 0.18241    |
| Perplexity           | 1.2001     |
| AveragePolicyProb[0] | 0.51173    |
| AveragePolicyProb[1] | 0.48827    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1007       |
| TotalNSamples        | 1.6625e+05 |
| ExplainedVariance    | 0.44712    |
-------------------------------------
[2019-08-24 14:38:44.990897 UTC] Saving snapshot
[2019-08-24 14:38:45.003576 UTC] Starting iteration 84
[2019-08-24 14:38:45.005995 UTC] Start collecting samples
[2019-08-24 14:38:45.292239 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:45.309236 UTC] Computing policy gradient
[2019-08-24 14:38:45.317967 UTC] Updating baseline
[2019-08-24 14:38:45.414880 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| SurrLoss             | 0.015674   |
| Entropy              | 0.18013    |
| Perplexity           | 1.1974     |
| AveragePolicyProb[0] | 0.50674    |
| AveragePolicyProb[1] | 0.49326    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1021       |
| TotalNSamples        | 1.6905e+05 |
| ExplainedVariance    | 0.70126    |
-------------------------------------
[2019-08-24 14:38:46.258812 UTC] Saving snapshot
[2019-08-24 14:38:46.272933 UTC] Starting iteration 85
[2019-08-24 14:38:46.274625 UTC] Start collecting samples
[2019-08-24 14:38:46.474835 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:46.488343 UTC] Computing policy gradient
[2019-08-24 14:38:46.495514 UTC] Updating baseline
[2019-08-24 14:38:46.574832 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| SurrLoss             | 0.0063141  |
| Entropy              | 0.1852     |
| Perplexity           | 1.2035     |
| AveragePolicyProb[0] | 0.50028    |
| AveragePolicyProb[1] | 0.49972    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1025       |
| TotalNSamples        | 1.6985e+05 |
| ExplainedVariance    | 0.6582     |
-------------------------------------
[2019-08-24 14:38:47.493201 UTC] Saving snapshot
[2019-08-24 14:38:47.507683 UTC] Starting iteration 86
[2019-08-24 14:38:47.509683 UTC] Start collecting samples
[2019-08-24 14:38:47.808345 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:47.825769 UTC] Computing policy gradient
[2019-08-24 14:38:47.835970 UTC] Updating baseline
[2019-08-24 14:38:47.952949 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| SurrLoss             | -0.013808  |
| Entropy              | 0.17897    |
| Perplexity           | 1.196      |
| AveragePolicyProb[0] | 0.49889    |
| AveragePolicyProb[1] | 0.50111    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1038       |
| TotalNSamples        | 1.7245e+05 |
| ExplainedVariance    | 0.79214    |
-------------------------------------
[2019-08-24 14:38:48.827891 UTC] Saving snapshot
[2019-08-24 14:38:48.841689 UTC] Starting iteration 87
[2019-08-24 14:38:48.844409 UTC] Start collecting samples
[2019-08-24 14:38:49.174351 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:49.189695 UTC] Computing policy gradient
[2019-08-24 14:38:49.197661 UTC] Updating baseline
[2019-08-24 14:38:49.298373 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| SurrLoss             | 0.007005   |
| Entropy              | 0.18276    |
| Perplexity           | 1.2005     |
| AveragePolicyProb[0] | 0.49716    |
| AveragePolicyProb[1] | 0.50284    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1051       |
| TotalNSamples        | 1.7505e+05 |
| ExplainedVariance    | 0.55669    |
-------------------------------------
[2019-08-24 14:38:50.181992 UTC] Saving snapshot
[2019-08-24 14:38:50.194390 UTC] Starting iteration 88
[2019-08-24 14:38:50.195977 UTC] Start collecting samples
[2019-08-24 14:38:50.458225 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:50.471389 UTC] Computing policy gradient
[2019-08-24 14:38:50.478023 UTC] Updating baseline
[2019-08-24 14:38:50.561189 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| SurrLoss             | 0.0073207  |
| Entropy              | 0.16911    |
| Perplexity           | 1.1842     |
| AveragePolicyProb[0] | 0.50635    |
| AveragePolicyProb[1] | 0.49365    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1056       |
| TotalNSamples        | 1.7605e+05 |
| ExplainedVariance    | 0.67203    |
-------------------------------------
[2019-08-24 14:38:51.471315 UTC] Saving snapshot
[2019-08-24 14:38:51.485517 UTC] Starting iteration 89
[2019-08-24 14:38:51.486810 UTC] Start collecting samples
[2019-08-24 14:38:51.823573 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:51.840961 UTC] Computing policy gradient
[2019-08-24 14:38:51.850338 UTC] Updating baseline
[2019-08-24 14:38:51.955444 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| SurrLoss             | -0.013039  |
| Entropy              | 0.15615    |
| Perplexity           | 1.169      |
| AveragePolicyProb[0] | 0.50077    |
| AveragePolicyProb[1] | 0.49923    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1069       |
| TotalNSamples        | 1.7865e+05 |
| ExplainedVariance    | 0.66265    |
-------------------------------------
[2019-08-24 14:38:53.134220 UTC] Saving snapshot
[2019-08-24 14:38:53.149716 UTC] Starting iteration 90
[2019-08-24 14:38:53.151012 UTC] Start collecting samples
[2019-08-24 14:38:53.418114 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:53.433118 UTC] Computing policy gradient
[2019-08-24 14:38:53.439977 UTC] Updating baseline
[2019-08-24 14:38:53.518557 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| SurrLoss             | 0.029525   |
| Entropy              | 0.16489    |
| Perplexity           | 1.1793     |
| AveragePolicyProb[0] | 0.49603    |
| AveragePolicyProb[1] | 0.50397    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1076       |
| TotalNSamples        | 1.8005e+05 |
| ExplainedVariance    | 0.65342    |
-------------------------------------
[2019-08-24 14:38:54.686527 UTC] Saving snapshot
[2019-08-24 14:38:54.701129 UTC] Starting iteration 91
[2019-08-24 14:38:54.702232 UTC] Start collecting samples
[2019-08-24 14:38:55.021124 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:55.037687 UTC] Computing policy gradient
[2019-08-24 14:38:55.047367 UTC] Updating baseline
[2019-08-24 14:38:55.133239 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| SurrLoss             | -0.0032053 |
| Entropy              | 0.16088    |
| Perplexity           | 1.1745     |
| AveragePolicyProb[0] | 0.49554    |
| AveragePolicyProb[1] | 0.50446    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1087       |
| TotalNSamples        | 1.8225e+05 |
| ExplainedVariance    | 0.67526    |
-------------------------------------
[2019-08-24 14:38:56.362295 UTC] Saving snapshot
[2019-08-24 14:38:56.377673 UTC] Starting iteration 92
[2019-08-24 14:38:56.379260 UTC] Start collecting samples
[2019-08-24 14:38:56.759769 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:56.778562 UTC] Computing policy gradient
[2019-08-24 14:38:56.787926 UTC] Updating baseline
[2019-08-24 14:38:56.883880 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| SurrLoss             | -0.012856  |
| Entropy              | 0.14707    |
| Perplexity           | 1.1584     |
| AveragePolicyProb[0] | 0.49622    |
| AveragePolicyProb[1] | 0.50378    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1101       |
| TotalNSamples        | 1.8505e+05 |
| ExplainedVariance    | 0.50678    |
-------------------------------------
[2019-08-24 14:38:57.937699 UTC] Saving snapshot
[2019-08-24 14:38:57.954045 UTC] Starting iteration 93
[2019-08-24 14:38:57.955432 UTC] Start collecting samples
[2019-08-24 14:38:58.216445 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:58.242175 UTC] Computing policy gradient
[2019-08-24 14:38:58.251317 UTC] Updating baseline
[2019-08-24 14:38:58.335187 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| SurrLoss             | 0.016223   |
| Entropy              | 0.1644     |
| Perplexity           | 1.1787     |
| AveragePolicyProb[0] | 0.49241    |
| AveragePolicyProb[1] | 0.50759    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1105       |
| TotalNSamples        | 1.8585e+05 |
| ExplainedVariance    | 0.54041    |
-------------------------------------
[2019-08-24 14:38:59.357627 UTC] Saving snapshot
[2019-08-24 14:38:59.371626 UTC] Starting iteration 94
[2019-08-24 14:38:59.374017 UTC] Start collecting samples
[2019-08-24 14:38:59.686703 UTC] Computing input variables for policy optimization
[2019-08-24 14:38:59.704344 UTC] Computing policy gradient
[2019-08-24 14:38:59.711690 UTC] Updating baseline
[2019-08-24 14:38:59.840274 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| SurrLoss             | -0.010354  |
| Entropy              | 0.14803    |
| Perplexity           | 1.1595     |
| AveragePolicyProb[0] | 0.5109     |
| AveragePolicyProb[1] | 0.4891     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1118       |
| TotalNSamples        | 1.8845e+05 |
| ExplainedVariance    | 0.15993    |
-------------------------------------
[2019-08-24 14:39:00.826295 UTC] Saving snapshot
[2019-08-24 14:39:00.839278 UTC] Starting iteration 95
[2019-08-24 14:39:00.841746 UTC] Start collecting samples
[2019-08-24 14:39:01.200321 UTC] Computing input variables for policy optimization
[2019-08-24 14:39:01.218599 UTC] Computing policy gradient
[2019-08-24 14:39:01.228164 UTC] Updating baseline
[2019-08-24 14:39:01.321680 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| SurrLoss             | 0.012306   |
| Entropy              | 0.15255    |
| Perplexity           | 1.1648     |
| AveragePolicyProb[0] | 0.50327    |
| AveragePolicyProb[1] | 0.49673    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1131       |
| TotalNSamples        | 1.9105e+05 |
| ExplainedVariance    | 0.45237    |
-------------------------------------
[2019-08-24 14:39:02.385427 UTC] Saving snapshot
[2019-08-24 14:39:02.401507 UTC] Starting iteration 96
[2019-08-24 14:39:02.402709 UTC] Start collecting samples
[2019-08-24 14:39:02.656651 UTC] Computing input variables for policy optimization
[2019-08-24 14:39:02.670962 UTC] Computing policy gradient
[2019-08-24 14:39:02.677410 UTC] Updating baseline
[2019-08-24 14:39:02.783097 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| SurrLoss             | -0.0063822 |
| Entropy              | 0.13737    |
| Perplexity           | 1.1472     |
| AveragePolicyProb[0] | 0.50604    |
| AveragePolicyProb[1] | 0.49396    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1136       |
| TotalNSamples        | 1.9205e+05 |
| ExplainedVariance    | 0.2063     |
-------------------------------------
[2019-08-24 14:39:03.828893 UTC] Saving snapshot
[2019-08-24 14:39:03.844232 UTC] Starting iteration 97
[2019-08-24 14:39:03.845406 UTC] Start collecting samples
[2019-08-24 14:39:04.180613 UTC] Computing input variables for policy optimization
[2019-08-24 14:39:04.216106 UTC] Computing policy gradient
[2019-08-24 14:39:04.225406 UTC] Updating baseline
[2019-08-24 14:39:04.326164 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| SurrLoss             | -0.0042873 |
| Entropy              | 0.13288    |
| Perplexity           | 1.1421     |
| AveragePolicyProb[0] | 0.49865    |
| AveragePolicyProb[1] | 0.50135    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1149       |
| TotalNSamples        | 1.9465e+05 |
| ExplainedVariance    | -0.10868   |
-------------------------------------
[2019-08-24 14:39:05.246343 UTC] Saving snapshot
[2019-08-24 14:39:05.259656 UTC] Starting iteration 98
[2019-08-24 14:39:05.260638 UTC] Start collecting samples
[2019-08-24 14:39:05.529797 UTC] Computing input variables for policy optimization
[2019-08-24 14:39:05.544193 UTC] Computing policy gradient
[2019-08-24 14:39:05.552114 UTC] Updating baseline
[2019-08-24 14:39:05.637278 UTC] Computing logging information
--------------------------------------
| Iteration            | 98          |
| SurrLoss             | -0.00075876 |
| Entropy              | 0.13356     |
| Perplexity           | 1.1429      |
| AveragePolicyProb[0] | 0.49975     |
| AveragePolicyProb[1] | 0.50025     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 1156        |
| TotalNSamples        | 1.9605e+05  |
| ExplainedVariance    | -0.20679    |
--------------------------------------
[2019-08-24 14:39:06.557012 UTC] Saving snapshot
[2019-08-24 14:39:06.571545 UTC] Starting iteration 99
[2019-08-24 14:39:06.573313 UTC] Start collecting samples
[2019-08-24 14:39:06.854474 UTC] Computing input variables for policy optimization
[2019-08-24 14:39:06.871088 UTC] Computing policy gradient
[2019-08-24 14:39:06.881030 UTC] Updating baseline
[2019-08-24 14:39:06.957362 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| SurrLoss             | -0.010159  |
| Entropy              | 0.13051    |
| Perplexity           | 1.1394     |
| AveragePolicyProb[0] | 0.49931    |
| AveragePolicyProb[1] | 0.50069    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1167       |
| TotalNSamples        | 1.9825e+05 |
| ExplainedVariance    | -0.031666  |
-------------------------------------
[2019-08-24 14:39:08.000256 UTC] Saving snapshot
